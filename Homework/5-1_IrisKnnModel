# 2-1. 사이킷런의 붓꽃(Iris) 데이터셋을 사용해서 K-최근접 이웃(K-NN) 모델을 학습하고 테스트 하는 코드

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1. 데이터 로드
iris = load_iris()
X, y = iris.data, iris.target

# 2. 데이터를 학습용과 테스트용으로 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. K-NN 모델 생성 및 학습
model = KNeighborsClassifier(n_neighbors=120) # 최대 120? 그 이후 오류
model.fit(X_train, y_train)

# 4. 모델 예측 및 평가
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"모델 정확도: {accuracy * 100:.2f}%")


# 더 좋은 방법
# 최적의 K값 찾기

# 1. 데이터 로드
iris = load_iris()
X, y = iris.data, iris.target

# 2. 학습용, 테스트용 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. 최적의 K값 찾기 (간단하게 탐색)
best_k = 1
best_score = 0

for k in range(1, len(X_train) + 1): # 1부터 훈련 데이터 개수까지
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)

    if score > best_score:
        best_k = k
        best_score = score
    
print(f"최적의 k: {best_k}, 정확도: {best_score * 100:.2f}%")

# 4. 최적의 K 로 최종 모델 학습 및 평가
model = KNeighborsClassifier(n_neighbors = best_k)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"최종 모델 정확도: {accuracy * 100:.2f}%")